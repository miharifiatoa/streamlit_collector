import pandas as pd
import json
import os
import argparse
import random
from tqdm import tqdm
import time
from pathlib import Path
import logging

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

def create_dataset(input_file, output_file, window_size=1, test_split=0.1, validate=True, seed=42):
    """
    Cr√©e un dataset au format JSONL pour l'entra√Ænement de mod√®les linguistiques.
    
    Args:
        input_file: Chemin vers le fichier de texte nettoy√©
        output_file: Chemin vers le fichier de sortie JSONL
        window_size: Nombre de lignes √† inclure comme contexte pour l'entr√©e
        test_split: Proportion des donn√©es √† r√©server pour le test (0-1)
        validate: Si True, effectue une validation des donn√©es g√©n√©r√©es
        seed: Graine pour la reproductibilit√©
    """
    start_time = time.time()
    
    # V√©rification de l'existence du fichier d'entr√©e
    input_path = Path(input_file)
    if not input_path.exists():
        raise FileNotFoundError(f"Fichier d'entr√©e introuvable: {input_file}")
    
    # Lecture du fichier d'entr√©e
    logger.info(f"Lecture du fichier {input_file}...")
    with open(input_file, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f.readlines() if line.strip()]
    
    logger.info(f"Cr√©ation du dataset avec {len(lines)} lignes de texte...")
    
    # Pr√©paration des donn√©es
    data = []
    
    # Utilisation de tqdm pour afficher une barre de progression
    for i in tqdm(range(len(lines) - window_size)):
        # Construction du contexte avec window_size lignes pr√©c√©dentes
        if window_size > 1:
            input_text = " ".join(lines[i:i+window_size])
        else:
            input_text = lines[i]
        
        output_text = lines[i + window_size]
        
        # V√©rification de la qualit√© des donn√©es
        if len(input_text) < 5 or len(output_text) < 5:
            continue  # Ignorer les entr√©es/sorties trop courtes
        
        data.append({
            "input": input_text,
            "output": output_text
        })
    
    # M√©lange des donn√©es pour la r√©partition train/test
    random.seed(seed)
    random.shuffle(data)
    
    # Division en ensembles d'entra√Ænement et de test
    split_idx = int(len(data) * (1 - test_split))
    train_data = data[:split_idx]
    test_data = data[split_idx:]
    
    # Cr√©ation des chemins de sortie
    output_path = Path(output_file)
    train_path = output_path.with_name(f"{output_path.stem}_train{output_path.suffix}")
    test_path = output_path.with_name(f"{output_path.stem}_test{output_path.suffix}")
    
    # Cr√©ation du r√©pertoire de sortie si n√©cessaire
    os.makedirs(output_path.parent, exist_ok=True)
    
    # √âcriture des fichiers JSONL
    logger.info(f"√âcriture de {len(train_data)} exemples d'entra√Ænement...")
    with open(train_path, 'w', encoding='utf-8') as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    logger.info(f"√âcriture de {len(test_data)} exemples de test...")
    with open(test_path, 'w', encoding='utf-8') as f:
        for item in test_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    # Statistiques des donn√©es
    input_lengths = [len(item["input"]) for item in data]
    output_lengths = [len(item["output"]) for item in data]
    
    # Validation du dataset (si demand√©)
    if validate:
        logger.info("Validation du dataset...")
        # Test de lecture du fichier JSONL g√©n√©r√©
        try:
            with open(train_path, 'r', encoding='utf-8') as f:
                first_lines = [json.loads(next(f)) for _ in range(min(5, len(train_data)))]
            logger.info(f"Validation r√©ussie. Exemple d'entr√©e: {first_lines[0]}")
        except Exception as e:
            logger.error(f"Erreur lors de la validation: {e}")
    
    processing_time = time.time() - start_time
    
    # Rapport final
    logger.info("‚úÖ Cr√©ation du dataset termin√©e")
    logger.info(f"‚è±Ô∏è Temps de traitement: {processing_time:.2f} secondes")
    logger.info(f"üìä Statistiques:")
    logger.info(f"  - Total: {len(data)} paires")
    logger.info(f"  - Entra√Ænement: {len(train_data)} exemples ‚Üí {train_path}")
    logger.info(f"  - Test: {len(test_data)} exemples ‚Üí {test_path}")
    logger.info(f"  - Longueur moyenne entr√©e: {sum(input_lengths)/len(input_lengths):.1f} caract√®res")
    logger.info(f"  - Longueur moyenne sortie: {sum(output_lengths)/len(output_lengths):.1f} caract√®res")
    
    return {
        "train_file": str(train_path),
        "test_file": str(test_path),
        "train_count": len(train_data),
        "test_count": len(test_data),
        "total_count": len(data)
    }

if __name__ == "__main__":
    # Configuration du parser d'arguments
    parser = argparse.ArgumentParser(description="Cr√©e un dataset au format JSONL pour l'entra√Ænement de mod√®les linguistiques")
    parser.add_argument("--input", default="data/clean/clean_texte_antandroy.txt", 
                        help="Chemin du fichier d'entr√©e nettoy√©")
    parser.add_argument("--output", default="data/processed/antandroy_dataset.jsonl", 
                        help="Chemin du fichier de sortie JSONL")
    parser.add_argument("--window", type=int, default=1, 
                        help="Nombre de lignes √† utiliser comme contexte")
    parser.add_argument("--test-split", type=float, default=0.1, 
                        help="Proportion des donn√©es √† r√©server pour le test (0-1)")
    parser.add_argument("--seed", type=int, default=42, 
                        help="Graine pour la reproductibilit√©")
    args = parser.parse_args()
    
    # Cr√©ation du dataset
    result = create_dataset(
        args.input, 
        args.output,
        window_size=args.window,
        test_split=args.test_split,
        seed=args.seed
    )